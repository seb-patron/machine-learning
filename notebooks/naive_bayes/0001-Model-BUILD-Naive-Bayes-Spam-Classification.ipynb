{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Spam Classifier\n",
    "\n",
    "Naive Bayes classifiers are a type of machine learning algorithm based applying [bayes theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) with strong (naive) independence assumptions between features. In short, a naive bayes classifier treats every features independent from each other, making inference very efficient. These types of classifiers are commonly used for spam detection.\n",
    "\n",
    "## Before we start...\n",
    "\n",
    "Let's quickly cover some of the basic definitions needed to understand our problem.\n",
    "\n",
    "### Bayes Theorem\n",
    "\n",
    "Bayes Theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event. Bayes Theorem is considered \"naive\" because it assumes that the presence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature. In other words, every feature is taken into account without considering the existence of another feature.\n",
    "\n",
    "Mathmatically, Bayes Theorem can be written as:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(B \\mid A) \\, P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Lets break this down:\n",
    "- $A$ and $B$ are considered seperate events, and the Probability of $B$ (ie $P(B)$) â‰  0.\n",
    "- $P(A)$ and $P(B)$ are the probabilities of observing events $A$ and $B$ without regard to each other.\n",
    "- $ P(A \\mid B) $ is the probability of observing event $A$ given that $B$ is true\n",
    "- $ P(B \\mid A) $ is the probability of observing event $B$ given that $A$ is true\n",
    "\n",
    "When applying Bayes Theorem to spam classification, we can rewrite the problem statment as:\n",
    "\n",
    "$$\n",
    "P(\\textrm{spam} \\mid \\textrm{w}1 \\cap \\textrm{w}2 \\> \\cap .. \\cap \\> \\textrm{w}n) = \\frac{P(\\textrm{w}1 \\cap \\textrm{w}2 \\> \\cap \\> .. \\cap \\> \\textrm{w}n \\mid \\textrm{spam}) \\, P(\\textrm{spam})}{P(\\textrm{w}1 \\cap \\textrm{w}2 \\> \\cap \\> .. \\cap \\> \\textrm{w}n)}\n",
    "$$\n",
    "\n",
    "Now, we have a message m that is made up of n number of words, or m = $ (w1 \\cap w2 \\cap .. \\cap wn) $. We assume the occurence of any word wn is independent of all other words.\n",
    "\n",
    "\n",
    "\n",
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 26 2018 \n",
      "\n",
      "CPython 3.7.1\n",
      "IPython 7.2.0\n",
      "\n",
      "numpy 1.15.4\n",
      "scipy 1.1.0\n",
      "sklearn 0.20.1\n",
      "pandas 0.23.4\n",
      "matplotlib 3.0.2\n",
      "\n",
      "compiler   : Clang 4.0.1 (tags/RELEASE_401/final)\n",
      "system     : Darwin\n",
      "release    : 18.0.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "/Users/sebp/LocalDocuments2/DataScience/Personal/MachineLearning/MachineLearning\n"
     ]
    }
   ],
   "source": [
    "# Loads watermark extension and prints details about current platform\n",
    "%load_ext watermark\n",
    "%watermark -v -n -m -p numpy,scipy,sklearn,pandas,matplotlib\n",
    "# autoreloads changes in imported files\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    " \n",
    "# import packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Get project directory\n",
    "PROJ_ROOT = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "print(PROJ_ROOT)\n",
    "import sys\n",
    "\n",
    "# make sure matplotlib will display inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read in Data\n",
    "\n",
    "We'll define a function that reads in our data and splits it into a training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    SMS_df = pd.read_csv(PROJ_ROOT +'/data/naive_bayes/raw/spam.csv',usecols=[0,1],encoding='latin-1')\n",
    "    SMS_df.columns=['label','content']\n",
    "    n = int(SMS_df.shape[0])\n",
    "    # split into training data and test data\n",
    "    return SMS_df.iloc[:int(n/2)], SMS_df.iloc[int(n/2):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate a Tabu List\n",
    "A **tabu list** is a list of those significant indicators of spam SMS. Here we select TF-IDF as the principle of list generation.\n",
    "\n",
    "Term Frequency(TF) is the frequency of a word in a certain kind of document. If there is a article of 50 words with 2 'data' in it, then the TF of the 'data' is given by 2/50=0.01.\n",
    "\n",
    "However, there are some words of high frequency in English, like 'a', 'is', 'are', etc. We have to remove those words from our list. We'll use IDF (inverse document frequency) to acomplish this task. Inverse Document Frequency is the indicator to reflect how important a word is related to some certain topic. It is given by log(#total articles/#articles containing w). The more common a word is accross all examples, the lower it's \"importance\" score will be.\n",
    "\n",
    "Here is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>cleaned_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, in, a, wkly, comp, to, win, fa, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                     cleaned_content  \n",
       "0  [go, until, jurong, point, crazy, available, o...  \n",
       "1                     [ok, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, in, a, wkly, comp, to, win, fa, ...  \n",
       "3  [u, dun, say, so, early, hor, u, c, already, t...  \n",
       "4  [nah, i, dont, think, he, goes, to, usf, he, l...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df,_ = read_data()\n",
    "\n",
    "train_df['cleaned_content'] = train_df.content.apply( lambda x: [i.lower() for i in re.findall('[A-Za-z]+', re.sub(\"'\",\"\",x))])\n",
    "train_df.head()\n",
    "\n",
    "# remove apostrophes in a way that won't use apostrophes as a word break\n",
    "# re.sub(\"'\",\"\",x)\n",
    "\n",
    "# find all words and return them as a list\n",
    "# re.findall('[A-Za-z]+'\n",
    "\n",
    "\n",
    "# train_df.content.apply( lambda x: [i.lower() for i in re.findall('[A-Za-z]+', re.sub(\"'\",\"\",x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tabu_list(path, tabu_size=200,ignore=3):\n",
    "    \"\"\"\n",
    "    path = file name to use for exporting list\n",
    "    tabu_size = length of exported list (ie how many rows through dataframe that function will process)\n",
    "    ignore = minimum word length necessary to process word (ie ignore common short words like a, at, the, etc)\n",
    "    \"\"\"\n",
    "    train_df,_ = read_data()\n",
    "    spam_TF_dict = dict()\n",
    "    valid_TF_dict = dict()\n",
    "    IDF_dict = dict()\n",
    "\n",
    "    # ignore all other than letters.\n",
    "    # returns list of words downcased, removing punctuation and anything that is not a letter\n",
    "    train_df['cleaned_content'] = train_df.content.apply( lambda x: [i.lower() for i in re.findall('[A-Za-z]+', re.sub(\"'\",\"\",x))])\n",
    "    \n",
    "#   # go through each word in the dataset and add it to a dict of \n",
    "    for i in range(train_df.shape[0]):\n",
    "#         finds = re.findall('[A-Za-z]+', train_df.iloc[i].content)\n",
    "        if train_df.iloc[i].label == 'spam':\n",
    "            for find in train_df.iloc[i].cleaned_content:\n",
    "                if len(find)<ignore: continue\n",
    "#                 find = find.lower()\n",
    "                try:\n",
    "                    # if the current word is already in our spam dict, increment the value (ie number of\n",
    "                    # occurences) by 1\n",
    "                    spam_TF_dict[find] = spam_TF_dict[find] + 1\n",
    "                except:\t\n",
    "                    # if the current word is not in our spam dict, add it, set the initial value to 1\n",
    "                    # and add the word to our valid dict and set the value to 0\n",
    "                    spam_TF_dict[find] = spam_TF_dict.get(find,1)\n",
    "                    valid_TF_dict[find] = valid_TF_dict.get(find,0)\n",
    "        else:\n",
    "            for find in train_df.iloc[i].cleaned_content:\n",
    "                if len(find)<ignore: continue\n",
    "#                 find = find.lower()\n",
    "                try:\n",
    "                    valid_TF_dict[find] = valid_TF_dict[find] + 1\n",
    "                except:\t\n",
    "                    spam_TF_dict[find] = spam_TF_dict.get(find,0)\n",
    "                    valid_TF_dict[find] = valid_TF_dict.get(find,1)\n",
    "\n",
    "        # basically just a list of each unique word\n",
    "        word_set = set()\n",
    "        for find in train_df.iloc[i].cleaned_content:\n",
    "            if len(find)<ignore: continue\n",
    "#             find = find.lower()\n",
    "            if not(find in word_set):\n",
    "                try:\n",
    "                    IDF_dict[find] = IDF_dict[find] + 1\n",
    "                except:\t\n",
    "                    IDF_dict[find] = IDF_dict.get(find,1)\n",
    "            word_set.add(find)\n",
    "\n",
    "    word_df = pd.DataFrame(list(zip(valid_TF_dict.keys(),valid_TF_dict.values(),spam_TF_dict.values(),IDF_dict.values())))\n",
    "    word_df.columns = ['keyword','valid_TF','spam_TF','IDF']\n",
    "    word_df['valid_TF'] = word_df['valid_TF'].astype('float')/train_df[train_df['label']=='ham'].shape[0]\n",
    "    word_df['spam_TF'] = word_df['spam_TF'].astype('float')/train_df[train_df['label']=='spam'].shape[0]\n",
    "    word_df['IDF'] = np.log10(train_df.shape[0]/word_df['IDF'].astype('float'))\n",
    "    word_df['valid_TFIDF'] = word_df['valid_TF']*word_df['IDF']\n",
    "    word_df['spam_TFIDF'] = word_df['spam_TF']*word_df['IDF']\n",
    "    word_df['diff']=word_df['spam_TFIDF']-word_df['valid_TFIDF']\n",
    "\n",
    "    selected_spam_key = word_df.sort_values('diff',ascending=False)\n",
    "\n",
    "    print('>>>Generating Tabu List...\\n  Tabu List Size: {}\\n  File Name: {}\\n  The words shorter than {} are ignored by model\\n'.format(tabu_size, path, ignore))\n",
    "    file = open(path,'w')\n",
    "    for word in selected_spam_key.head(tabu_size).keyword:\n",
    "        file.write(word+'\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Read Tabu List and Convert SMS\n",
    "Since the message is of variant length, it is not easy for the implementation of learning algorithm. So we define a Function above generating tabu list and storing them in the local file. And we can use this file to convert a SMS expressed in string to a vector of fixed length expressed in binary value.\n",
    "\n",
    "The idea is given like this: If we have a tabu list then we could find those word in the list and represent them by a index. Thus a string can be converted to an array of int. Further, we could define an array filled with zeros with the same length of tabu list. if this str contains the word in the tabu list, we could assign 1 to the corresponding element of the array representing 'message contains word w'. (tips: the query of python.dict is of constant time, much faster than python.list)\n",
    "\n",
    "By taking this step, we could convert our raw data of variant length into the numeric data of fixed length.\n",
    "\n",
    "These two function is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tabu_list(path):\n",
    "    file = open(path,'r')\n",
    "    keyword_dict = dict()\n",
    "    i = 0\n",
    "    for line in file:\n",
    "        keyword_dict.update({line.strip():i})\n",
    "        i+=1\n",
    "    return keyword_dict\n",
    "\n",
    "# create a numpy array of length tabu, ie the number of unique words\n",
    "# go through each word passed in 'content' (content is a string of words)\n",
    "# for each unique word in the string, find it's index in the numpy array,\n",
    "# and set its value to '1' to show it exists\n",
    "def convert_content(content, tabu):\n",
    "    m = len(tabu)\n",
    "    res = np.int_(np.zeros(m))\n",
    "    finds = re.findall('[A-Za-z]+', content)\n",
    "    for find in finds:\n",
    "        find=find.lower()\n",
    "        try:\n",
    "            i = tabu[find]\n",
    "            res[i]=1\n",
    "        except:\n",
    "            continue\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Learning, Testing and Predicting\n",
    "After we generate our tabu list and those supporting functions, we are now well prepared for the learning part in this problem. And here we could use the library from sklearn.naive_bayes import BernoulliNB. It will help us train this model.\n",
    "\n",
    "Before this part, let review our data: our feature input X is a nm matrix, where X[i,j] = 1 means the sample #i contains the word j in the tabu list, and supervised label Y is a n1 vector where Y[i] = 1 representing for a spam and 0 for a ham.\n",
    "\n",
    "Let prepare the materials for the learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn():\n",
    "    global tabu, m\n",
    "    train,_ = read_data()\n",
    "    n = train.shape[0]\n",
    "    X = np.zeros((n,m)); Y=np.int_(train.label=='spam')\n",
    "    for i in range(n):\n",
    "        X[i,:] = convert_content(train.iloc[i].content, tabu)\n",
    "\n",
    "    NaiveBayes = BernoulliNB()\n",
    "    NaiveBayes.fit(X, Y)\n",
    "\n",
    "    Y_hat = NaiveBayes.predict(X)\n",
    "    print('>>>Learning...\\n  Learning Sample Size: {}\\n  Accuarcy (Training sample): {:.2f}ï¼…\\n'.format(n,sum(np.int_(Y_hat==Y))*100./n))\n",
    "    return NaiveBayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Function above returns a well trained Naive Bayes Model object, and we could use it to make prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(NaiveBayes):\n",
    "    global tabu, m\n",
    "    _,test = read_data()\n",
    "    n = test.shape[0]\n",
    "    X = np.zeros((n,m)); Y=np.int_(test.label=='spam')\n",
    "    for i in range(n):\n",
    "        X[i,:] = convert_content(test.iloc[i].content, tabu)\n",
    "    Y_hat = NaiveBayes.predict(X)\n",
    "    print ('>>>Cross Validation...\\n  Testing Sample Size: {}\\n  Accuarcy (Testing sample): {:.2f}ï¼…\\n'.format(n,sum(np.int_(Y_hat==Y))*100./n))\n",
    "    return\n",
    "\n",
    "def predictSMS(SMS):\n",
    "    global NaiveBayes, tabu, m\n",
    "    X = convert_content(SMS, tabu)\n",
    "    Y_hat = NaiveBayes.predict(X.reshape(1,-1))\n",
    "    if int(Y_hat) == 1:\n",
    "        print ('SPAM: {}'.format(SMS))\n",
    "    else:\n",
    "        print ('HAM: {}'.format(SMS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Overall Assembly\n",
    "After we define the every modules we need in this problem, we could integrate them into a whole part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCI SMS SPAM CLASSIFICATION PROBLEM SET\n",
      "  -- implemented by Bernoulli Naive Bayes Model\n",
      "\n",
      ">>>Generating Tabu List...\n",
      "  Tabu List Size: 300\n",
      "  File Name: /Users/sebp/LocalDocuments2/DataScience/Personal/MachineLearning/MachineLearning/data/naive_bayes/interim/tabu.txt\n",
      "  The words shorter than 3 are ignored by model\n",
      "\n",
      ">>>Learning...\n",
      "  Learning Sample Size: 2786\n",
      "  Accuarcy (Training sample): 98.31ï¼…\n",
      "\n",
      ">>>Cross Validation...\n",
      "  Testing Sample Size: 2786\n",
      "  Accuarcy (Testing sample): 97.85ï¼…\n",
      "\n",
      ">>>Testing\n",
      "HAM: Ya very nice. . .be ready on thursday\n",
      "SPAM: Had your mobile 10 mths? Update to the latest Camera/Video phones for FREE. KEEP UR SAME NUMBER, Get extra free mins/texts. Text YES for a call\n"
     ]
    }
   ],
   "source": [
    "print('UCI SMS SPAM CLASSIFICATION PROBLEM SET\\n  -- implemented by Bernoulli Naive Bayes Model\\n')\n",
    "tabu_file = PROJ_ROOT + '/data/naive_bayes/interim/tabu.txt'          # user defined tabu file\n",
    "tabu_size = 300                 # how many features are used to classify spam\n",
    "word_len_ignored = 3            # ignore those words shorter than this variable\n",
    "# build a tabu list based on the training data\n",
    "generate_tabu_list(tabu_file,tabu_size, word_len_ignored)\n",
    "\n",
    "tabu = read_tabu_list(tabu_file)\n",
    "m = len(tabu)\n",
    "# train the Naive Bayes Model using training data\n",
    "NaiveBayes=learn()\n",
    "# Test Model using testing data\n",
    "test(NaiveBayes)\n",
    "print('>>>Testing')\n",
    "# I select two messages from the test data here.\n",
    "predictSMS('Ya very nice. . .be ready on thursday')\n",
    "predictSMS('Had your mobile 10 mths? Update to the latest Camera/Video phones for FREE. KEEP UR SAME NUMBER, Get extra free mins/texts. Text YES for a call')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-ml",
   "language": "python",
   "name": "conda-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
